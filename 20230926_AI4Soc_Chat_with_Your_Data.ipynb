{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAhBW4_JFsv"
      },
      "source": [
        "# Chat based document Q&A with LangChain and OpenAI API\n",
        "\n",
        "This notebook will guide you through the process of setting up the environment, importing documents, and interacting with LangChain for document-based Q&A. We'll cover topics such as document preprocessing, question formulation, and analyzing the model's responses. This technique is called [Retrieval Augmented Generation (RAG)](https://www.perplexity.ai/search/What-is-Retrieval-s56y5BPbQGyxsvOU._R6ng) - [original paper](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "Whether you're a researcher, student, or professional, this demo notebook will showcase how LangChain can revolutionize your document exploration and information retrieval workflows.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/aiaicaramba/aiai-for-society/blob/aiai-dev/20230926_AI4Soc_Chat_with_Your_Data.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKfWq0yAiQSv"
      },
      "outputs": [],
      "source": [
        "# cleanup mess and setup data dir\n",
        "!rm -drf /content/sample_data\n",
        "!mkdir /content/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ3ah-Fi99HT"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "This code installs several Python packages that are required for the project. Explanations happily generated for you by [Chepetto](https://openai.com/blog/chatgpt).\n",
        "\n",
        "- [`langchain`](<https://python.langchain.com/>) is a package for language modeling and language generation tasks.\n",
        "- [`openai`](<https://openai.com/>) is a package for accessing the OpenAI API, which provides access to various language models and AI tools.\n",
        "- [`pypdf`](<https://pypi.org/project/PyPDF2/>) is a package for working with PDF files in Python.\n",
        "- [`tiktoken`](<https://github.com/openai/tiktoken>) is a package for accessing the TikToken API.\n",
        "- [`faiss-cpu`](<https://github.com/facebookresearch/faiss>) is a package for performing efficient similarity searches on large datasets using the FAISS library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dgzZqejc_tHY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in ./.venv/lib/python3.11/site-packages (0.0.327)\n",
            "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (0.28.1)\n",
            "Requirement already satisfied: pypdf in ./.venv/lib/python3.11/site-packages (3.17.0)\n",
            "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.11/site-packages (1.7.4)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.1.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tiktoken in ./.venv/lib/python3.11/site-packages (0.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.11/site-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.11/site-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in ./.venv/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.11/site-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in ./.venv/lib/python3.11/site-packages (from langchain) (0.0.56)\n",
            "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (1.26.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (2.4.2)\n",
            "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3)\n",
            "Collecting pyarrow>=8.0.0 (from datasets)\n",
            "  Downloading pyarrow-13.0.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Collecting filelock (from huggingface-hub<1.0.0,>=0.14.0->datasets)\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-13.0.0-cp311-cp311-macosx_10_14_x86_64.whl (25.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp311-cp311-macosx_10_9_x86_64.whl (31 kB)\n",
            "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: xxhash, pyarrow, fsspec, filelock, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.18.0 multiprocess-0.70.15 pyarrow-13.0.0 xxhash-3.4.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \\\n",
        "langchain \\\n",
        "openai \\\n",
        "pypdf \\\n",
        "faiss-cpu \\\n",
        "pandas \\\n",
        "datasets \\\n",
        "tiktoken "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g3qnWtL9uoZ"
      },
      "source": [
        "# OpenAI API Key\n",
        "\n",
        "To use the OpenAI API, you need to obtain an API key from the [OpenAI website](https://platform.openai.com/account/api-keys). The API key is a unique identifier that allows you to access the OpenAI API and make requests to it. By setting the 'OPENAI_API_KEY' environment variable, you can securely provide your API key to the code without hardcoding it into the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DAlyjKew6n0N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyqRRBW89pef"
      },
      "source": [
        "# Embeddings setup\n",
        "\n",
        "This code initializes an instance of the [OpenAIEmbeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html?highlight=embeddings#langchain.embeddings.OpenAIEmbeddings) class and assigns it to the variable embeddings. An [embedding](https://platform.openai.com/docs/guides/embeddings) is a way to represent words or phrases as numeric vectors, which can be used as input to machine learning models.  The `OpenAIEmbeddings` class provides access to pre-trained word embeddings from OpenAI, which were trained on a large corpus of text data using advanced deep learning techniques.\n",
        "\n",
        "Once you have initialized an instance of the `OpenAIEmbeddings` class, you can use it to obtain the embedding vector for any given chunk of text. This can be useful for a variety of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) tasks, such as sentiment analysis, language translation, and text classification. In this notebook we use it to do [semantic search](https://en.wikipedia.org/wiki/Semantic_search) with a [vector database](https://www.youtube.com/watch?v=klTvEwg3oJ4&ab_channel=Fireship) in this case.\n",
        "\n",
        "## Model\n",
        "\n",
        "| Name | Tokenizer | Max input tokens | Output dimensions |\n",
        "| :--- | :--- | ---: | ---: |\n",
        "| text-embedding-ada-002 | cl100k_base | 8191 | 1536 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ywZY3ISvBtG0"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQndn4wG9gmV"
      },
      "source": [
        "# Splitter setup\n",
        "\n",
        "The [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html) is a text splitting tool that takes in a large text document as input and splits it into smaller chunks for downstream processing. Here's what each parameter in the splitter setup means:\n",
        "\n",
        "- `chunk_size`: This parameter specifies the size of each chunk of text that the splitter will output. In this case, the splitter is set up to output chunks of 500 characters each.\n",
        "\n",
        "- `chunk_overlap`: This parameter specifies the number of characters of overlap that each chunk will have with the next chunk. In this case, the splitter is set up to have an overlap of 20 characters between adjacent chunks.\n",
        "\n",
        "- `length_function`: This parameter specifies the function that the splitter will use to calculate the length of the input text. In this case, the `len` function is used, which returns the number of characters in the text.\n",
        "\n",
        "Together, these parameters determine how the input text will be split into smaller chunks. The splitter will output chunks of 500 characters each, with an overlap of 20 characters between adjacent chunks, until the entire input text has been processed. This setup is designed to balance the need for small enough chunks for efficient processing, with enough overlap between chunks to minimize the risk of losing contextual information at the boundaries between chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oXNfZfbW-AwH"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrQCZBg9bjo"
      },
      "source": [
        "# Load (and split) documents\n",
        "\n",
        "This code snippet loads PDF files from a directory named \"data/\" using a [PyPDFDirectoryLoader](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html?highlight=PyPDFDirectoryLoader) class from the `langchain.document_loaders` module. The `loader` variable is an instance of `PyPDFDirectoryLoader`, which takes the directory path as an argument.\n",
        "\n",
        "After instantiating the loader, the code calls the `load_and_split` method to load the PDF files from the directory and split their text using the text splitter we created before.\n",
        "\n",
        "## Upload your PDFs\n",
        "Create a folder called 'data' and throw in any number of pdf's you'd like to chat with.\n",
        "\n",
        "> Note; The pdf's will be deleted once you close the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PLbGxELv8X4C"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "loader = PyPDFDirectoryLoader(\"data/\")\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "len(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "imTLGCkO14Qd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'17.3 Ik een beetje meer dan jij…\\nJe verdeelt een geldbedrag over vier personen (A, B, C en D). Elke persoon krijgt een ander\\nbedrag: A krijgt 20% meer dan het gemiddelde van de vier personen. Voor de duidelijkheid:\\nuiteindelijk krijgen de vier personen samen het hele bedrag. A krijgt daarvan een kwart +\\n20% van zo’n kwart. Daarna verlaat persoon A de kamer.\\nEr blijven drie personen over, met wie een soortgelijke verdeling plaatsvindt: B krijgt 20%\\nmeer dan het gemiddelde van de 3 overgebleven personen en verlaat de kamer. C krijgt 20%\\nmeer dan het gemiddelde van de 2 overgebleven personen. D ontvangt de overgebleven 168\\neuro.\\nHet oorspronkelijke, te verdelen bedrag is … euro.\\nSchrijf een computerprogramma dat dit getal berekent.\\nHieronder een deel van de oplossing, maar probeer het eerst zelf! (Afkomstig van beterreken.nl,\\n12aug2020; een som van Henk van Huffelen)\\n17.4 Ik een beetje meer dan jij… aanzet tot een oplossing'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[111].page_content # arbitrary chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zEAnfzs4oBUx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"17.3 Ik een beetje meer dan jij… Je verdeelt een geldbedrag over vier personen (A, B, C en D). Elke persoon krijgt een ander bedrag: A krijgt 20% meer dan het gemiddelde van de vier personen. Voor de duidelijkheid: uiteindelijk krijgen de vier personen samen het hele bedrag. A krijgt daarvan een kwart + 20% van zo'n kwart. Daarna verlaat persoon A de kamer. Er blijven drie personen over, met wie een soortgelijke verdeling plaatsvindt: B krijgt 20% meer dan het gemiddelde van de 3 overgebleven personen en verlaat de kamer. C krijgt 20% meer dan het gemiddelde van de 2 overgebleven personen. D ontvangt de overgebleven 168 euro. Het oorspronkelijke, te verdelen bedrag is … euro. Schrijf een computerprogramma dat dit getal berekent. Hieronder een deel van de oplossing, maar probeer het eerst zelf! (Afkomstig van beterreken.nl, 12aug2020; een som van Henk van Huffelen) 17.4 Ik een beetje meer dan jij… aanzet tot een oplossing\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Do a little cleanup.\n",
        "# Generated by GPT4 advanced data analysis: https://chat.openai.com/share/237b1e56-349e-4a5c-a007-d862c724b72f\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Step 1: Remove extra spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # Step 2: Normalize punctuation by removing unnecessary spaces before or after\n",
        "    text = re.sub(' ,', ',', text)\n",
        "    text = re.sub(' \\.', '.', text)\n",
        "    text = re.sub(' ;', ';', text)\n",
        "    text = re.sub(' !', '!', text)\n",
        "    text = re.sub(' \\?', '?', text)\n",
        "    text = re.sub('“ ', '“', text)\n",
        "    text = re.sub(' ”', '”', text)\n",
        "\n",
        "    # Step 3: Remove line breaks\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    return text\n",
        "\n",
        "def ultimate_clean_text(text):\n",
        "    # Previous cleaning steps\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Replacing various types of quotes with standard ASCII quotes\n",
        "    text = text.replace('“', '').replace('”', '').replace('‘', \"'\").replace('’', \"'\").replace('www.ztcprep.com', '')\n",
        "\n",
        "    # Replacing non-breaking spaces with standard spaces\n",
        "    text = text.replace(u'\\xa0', u' ')\n",
        "\n",
        "    # Correcting contractions like \"don' t\" to \"don't\"\n",
        "    text = re.sub(r\"(\\w+) \\'\", r\"\\1'\", text)\n",
        "    text = re.sub(r\"\\' t\", \"'t\", text)\n",
        "\n",
        "    # Removing website URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "cleaned_contents = list(map(lambda doc: ultimate_clean_text(doc.page_content), docs))\n",
        "cleaned_contents[111]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq0xnWT79I0B"
      },
      "source": [
        "# Vector store setup\n",
        "\n",
        "This code creates a **Facebook AI Similarity Searck** ([FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)) from the [langchain.vectorstores](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) module.\n",
        "\n",
        "FAISS enables fast similarity search among high-dimensional vectors. In this code, it is used to create a FAISS index for a set of documents. The `from_documents()` method is called on the FAISS class, passing in two arguments: `docs` and `embeddings`. The `docs` parameter is a set of documents that have been previously loaded and split, and the `embeddings` parameter is a pre-trained embedding model that maps the documents to high-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "umIO18tb8tpz"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "faiss_index = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osmwb3QL9QiX"
      },
      "source": [
        "# Save the db\n",
        "\n",
        "This code saves the FAISS index created in the previous code cell to disk with the name `faiss_index`. The `save_local()` method is called on the `faiss_index` object, which is the FAISS index created earlier. The `save_local()` method is a utility method provided by the FAISS class to save the index to the local file system.\n",
        "\n",
        "After executing this code, a file named \"faiss_index\" should be created in the current working directory. This file contains the serialized version of the FAISS index, which can be loaded back into memory later using the `FAISS.load_local()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M3AVw_PWCEg-"
      },
      "outputs": [],
      "source": [
        "faiss_index.save_local(\"faiss_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3AVw_PWCEg-"
      },
      "source": [
        "# Test the vector store\n",
        "\n",
        "This code performs a similarity search using the FAISS index created earlier and the query string *\"Is a prototyping more than enough for software?\"*.\n",
        "\n",
        "The `similarity_search()` method is called on the faiss_index object with two arguments: the query string and `k=5`, which specifies that the top 5 most similar documents should be returned. The result of the similarity search is stored in the `query_result` variable.\n",
        "\n",
        "The code then iterates over the chunks in the `query_result` list and prints the metadata and page content of each `chunk`. Specifically, it prints the page number and source of the document, along with its page content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5sRA2Fe9Tky"
      },
      "source": [
        "# Test the vector store\n",
        "\n",
        "This code performs a similarity search using the FAISS index created earlier and the query string *\"Is a prototyping more than enough for software?\"*.\n",
        "\n",
        "The `similarity_search()` method is called on the faiss_index object with two arguments: the query string and `k=5`, which specifies that the top 5 most similar documents should be returned. The result of the similarity search is stored in the `query_result` variable.\n",
        "\n",
        "The code then iterates over the chunks in the `query_result` list and prints the metadata and page content of each `chunk`. Specifically, it prints the page number and source of the document, along with its page content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lV2Pmvrt_FTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77 data/Programming-Basics.pdf : Bronnen: Een aantal van deze opgaven komen van www.beterrekenen.nl (ook als mobile app):\n",
            "Elke dag een paar minuten aan uitdagingen, soms makkelijk, soms moeilijker, some recht-\n",
            "toe-recht-aan, soms echt nodig om een plaatje te tekenen of een laptop te hulp te roepen…\n",
            "hersengym dus… een aanrader!\n",
            "17.7 Spoilers…\n",
            "Enkele antwoorden:\n",
            "•Al wandelend... : 4746 meter\n",
            "78\n",
            "116 data/Programming-Basics.pdf : abelen (en namen van lokale variabelen beginnen met een kleineletter).\n",
            "•DeMain-method is de enige plek waar Console.ReadLine() enConsole.WriteLine()\n",
            "voorkomt!\n",
            "•Roep elke methode meerdere keren aan! Bijvoorbeeld een methode die 2 strings als\n",
            "parameter heeft roep je minstens 1 keer aan met 2 strings van verschillende lengte, maar\n",
            "ook met 2 strings van gelijke lengte. Controleer het antwoord!\n",
            "•Dehappy flow testen we tijdens het programmeren meestal wel. Probeer ook uitzon-\n",
            "deringssituaties te testen! Geef eens een lege string mee aan een methode die een string-\n",
            "parameter heeft, en een methode die een List of een array verwacht wordt aangeroepen\n",
            "met een lege List of array.\n",
            "42.2 Eerste Hulp Bij Vasthangen\n",
            "Hoe pulk je letters uit een string? Hoe tel je het aantal letters in een string? Hoe kijk je of\n",
            "een string als deel van een andere string voorkomt? + string manipulatie (in Toolbox basis)\n",
            "117\n",
            "25 data/Programming-Basics.pdf : Step Into laat regel voor regel door de code stappen. Dit betekent dat je ook in een methode\n",
            "kunt gaan die worden aangeroepen.\n",
            "Stap met F11 door deze code heen. In het Command Window zie je het resultaat hiervan\n",
            "weergegeven.\n",
            "Step Over laat je ook stap voor stap door de code lopen maar gaat niet de methodes in. Dat\n",
            "is handig als je alleen het resultaat van een methode wilt zien. Dat kan is onze code niet\n",
            "bekeken worden, we roepen namelijk geen andere methode aan.\n",
            "26\n",
            "26 data/Programming-Basics.pdf : Step Out is handig als je niet langer door een methode wilt lopen. Dat heb je nodig als je\n",
            "in een systeemfunctie bent beland. Maar ook als je gewoon snel naar het volgende statement\n",
            "wilt kun je deze knop gebruiken.\n",
            "Continue is handig als je tijdens het debuggen gezien hebt wat je moet zien. Dus als je al\n",
            "gezien hebt dat het goed gaat. Als je op Continue klikt gaat de debugger naar het volgende\n",
            "breakpoint of naar het einde van het programma als er geen breakpoint meer is.\n",
            "5.3 Breakpoints\n",
            "BreakPoints zijn erg belangrijk bij debuggen. Je kunt een BreakPoint ook zetten door\n",
            "éénmaal op de grijze verticale balk aan de linkerkant van je scherm te klikken.\n",
            "BreakPoints opent het BreakPoint Window en integreert het samen met andere windows\n",
            "onderaan in de Visual Studio omgeving. Dit window bevat informatie over alle BreakPoints\n",
            "die op dit moment zijn gezet.\n",
            "Zet in je programma ook nog een BreakPoint op methodenaam “Main” en bekijk het\n",
            "BreakPoints Window .\n",
            "27\n",
            "73 data/Programming-Basics.pdf : 17 Variabelen, bewerkingen en conversies:\n",
            "T raining Computer-rekenen\n",
            "Als de computer dan toch zo’n krachtige calculator is, laten we hem dan ook als zodanig\n",
            "gebruiken…\n",
            "17.1 Recht-toe-recht-aan\n",
            "•100 x 0,7 - 25 x 0,6 = …\n",
            "•18.0 / 5.0 = …\n",
            "•100 / 30 = …\n",
            "Schrijf een programma dat deze antwoorden berekent.\n",
            "Als je het op een rekenmachine (bijvoorbeeld de calculator-app op je laptop) berekent: komt\n",
            "er dan dezelfde uitkomst uit? Zo nee, probeer erachter te komen waarom dat zo is.\n",
            "17.2 Al wandelend…\n",
            "Patrick maakt een wandeling van A naar B. De afstand is 20 km.\n",
            "Hij begint in A en als hij 1/4 deel van de afstand heeft afgelegd, rust hij een poosje (rustpunt\n",
            "1). Na korte tijd hervat hij de wandeling en loopt 1/4 deel van het resterende gedeelte, Daar\n",
            "rust hij weer even uit (rustpunt 2). Op dezelfde manier wandelt hij verder. Elke keer rust hij\n",
            "een poosje als hij weer 1/4 deel van het resterende gedeelte heeft gelopen.\n"
          ]
        }
      ],
      "source": [
        "query_result = faiss_index.similarity_search(\"Wat is een loop?\", k=5)\n",
        "\n",
        "for chunk in query_result:\n",
        "    print(str(chunk.metadata[\"page\"]) + \" \" + chunk.metadata[\"source\"] + \" :\", chunk.page_content[:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncBRUXkX8xrH"
      },
      "source": [
        "# Chat memory\n",
        "\n",
        "This code imports the [ConversationBufferWindowMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer_window.html) class from the `langchain.memory` module and creates an instance of it called `memory`. This class represents a memory buffer that stores conversations in a windowed fashion, meaning that the buffer only retains a certain number of recent conversations.\n",
        "\n",
        "The constructor of the `ConversationBufferWindowMemory` class takes two arguments: `memory_key` and `return_messages`. The `memory_key` parameter specifies a unique identifier for the memory buffer, and the `return_messages` parameter indicates whether or not to return the stored messages along with their metadata when accessing the memory buffer.\n",
        "\n",
        "In this code, the `memory_key` is set to \"chat_history\", which is being used to store the chat conversations. The return_messages parameter is set to `True`, which indicates that the stored messages will be returned along with their metadata when accessing the memory buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hWbEn5NlBWK7"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkpI1EY8t02"
      },
      "source": [
        "# Chain setup\n",
        "\n",
        "This code imports several classes and functions from various modules in the langchain package and creates an instance of the [ConversationalRetrievalChain](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html?highlight=ConversationalRetrievalChain) class called `qa`.\n",
        "\n",
        "The `ConversationalRetrievalChain` class is a high-level class that provides an interface for building a conversational agent that can perform retrieval-based question answering. In this code, the `qa` instance is initialized using the `from_llm()` method, which initializes the agent using an LLM model, a retriever and the memory buffer.\n",
        "\n",
        "### LLM\n",
        "The `OpenAI` class from the `langchain.llms` module represents an instance of the OpenAI language model. In this code, an instance of the OpenAI class is created of the model \"[gpt-3.5-turbo](https://platform.openai.com/docs/models)\".\n",
        "\n",
        "### Vector Store\n",
        "The `faiss_index.as_retriever()` method returns a retriever instance that wraps the FAISS index created earlier. This retriever is used to retrieve candidate answers to questions asked of the conversational agent.\n",
        "\n",
        "### Chat History Memory\n",
        "The `memory` variable is a memory buffer that was created earlier using the `ConversationBufferWindowMemory` class. This memory buffer is used to store and retrieve past conversations for use in future interactions.\n",
        "\n",
        "The `verbose=True` parameter indicates that verbose output should be produced when running the conversational agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "--P_eVczGm1X"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0.7, max_tokens=500),\n",
        "    faiss_index.as_retriever(k=5),\n",
        "    memory=memory, verbose=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "source": [
        "Voorbeeld: \n",
        "queries = ['Which school does Harry Potter attend and why?',\n",
        "         'How did Harry found out about his powers and what are they?',\n",
        "         'How did he become friends with the other students?',\n",
        "         'Describe the character Hermione.']\n",
        "\n",
        "chat_result = qa({\"question\": queries[0]})  # De 1e vraag wordt beantwoord (want index begint bij 0)\n",
        "chat_result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some example queries: \n",
            "0 : Hoe maak ik een loop die start bij 3 en loopt tot 25 0 Hoe maak ik een loop die start bij 3 en loopt tot 25\n",
            "1 : hoe verander ik het returntype van een methode? 1 hoe verander ik het returntype van een methode?\n",
            "2 : Kan ik in C# ook modulo berekenen? 2 Kan ik in C# ook modulo berekenen?\n",
            "3 : Hoe bepaal ik in C# of een getal deelbaar is door 7? 3 Hoe bepaal ik in C# of een getal deelbaar is door 7?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Your input: [hoe bepaal ik of een getal een priemgetal is in C#?]\n",
            "Ah, you typed a nice question.\n",
            "Q: hoe bepaal ik of een getal een priemgetal is in C#?\n",
            "Om te bepalen of een getal een priemgetal is in C#, kun je een functie schrijven die het getal controleert op delers. Een priemgetal is namelijk een getal dat alleen deelbaar is door 1 en zichzelf.\n",
            "\n",
            "Hier is een voorbeeld van een functie die controleert of een getal een priemgetal is:\n",
            "\n",
            "```csharp\n",
            "public static bool IsPrime(int number)\n",
            "{\n",
            "    if (number <= 1)\n",
            "    {\n",
            "        return false;\n",
            "    }\n",
            "\n",
            "    for (int i = 2; i <= Math.Sqrt(number); i++)\n",
            "    {\n",
            "        if (number % i == 0)\n",
            "        {\n",
            "            return false;\n",
            "        }\n",
            "    }\n",
            "\n",
            "    return true;\n",
            "}\n",
            "```\n",
            "\n",
            "Je kunt deze functie gebruiken door het gewenste getal als argument aan de functie door te geven. De functie retourneert `true` als het getal een priemgetal is en `false` als het geen priemgetal is.\n",
            "\n",
            "Hier is een voorbeeld van hoe je de functie kunt gebruiken:\n",
            "\n",
            "```csharp\n",
            "int number = 17;\n",
            "bool isPrime = IsPrime(number);\n",
            "\n",
            "if (isPrime)\n",
            "{\n",
            "    Console.WriteLine($\"{number} is een priemgetal.\");\n",
            "}\n",
            "else\n",
            "{\n",
            "    Console.WriteLine($\"{number} is geen priemgetal.\");\n",
            "}\n",
            "```\n",
            "\n",
            "Dit voorbeeld controleert of het getal 17 een priemgetal is. De uitvoer zou moeten zijn: \"17 is een priemgetal.\"\n"
          ]
        }
      ],
      "source": [
        "queries = ['Hoe maak ik een loop die start bij 3 en loopt tot 25',\n",
        "         'hoe verander ik het returntype van een methode?',\n",
        "         'Kan ik in C# ook modulo berekenen?',\n",
        "         'Hoe bepaal ik in C# of een getal deelbaar is door 7?']\n",
        "\n",
        "print(\"Some example queries: \")\n",
        "for nr,q in enumerate(queries):\n",
        "    print(f\"{ nr } : {q}\",nr,q)\n",
        "\n",
        "\n",
        "q = input(\"Typ number of query, or type in your own query, then press <enter>\")\n",
        "print(f\"\\nYour input: [{q}]\")\n",
        "if q.isdigit():\n",
        "    q = int(q)\n",
        "    if q in range(len(queries)):\n",
        "        q = queries[q]\n",
        "    else:\n",
        "        print(\"I don't know what question that would be...\")\n",
        "else:\n",
        "    print(\"Ah, you typed a nice question.\")\n",
        "    # q probably already contains the query typed by the user.\n",
        "\n",
        "# q is a string containing the chosen query\n",
        "\n",
        "print(f\"Q: {q}\")\n",
        "chat_result = qa({\"question\": q})\n",
        "print(chat_result['answer'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
