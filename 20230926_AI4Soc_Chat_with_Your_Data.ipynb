{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAhBW4_JFsv"
      },
      "source": [
        "# Chat based document Q&A with LangChain and OpenAI API\n",
        "\n",
        "This notebook will guide you through the process of setting up the environment, importing documents, and interacting with LangChain for document-based Q&A. We'll cover topics such as document preprocessing, question formulation, and analyzing the model's responses. This technique is called [Retrieval Augmented Generation (RAG)](https://www.perplexity.ai/search/What-is-Retrieval-s56y5BPbQGyxsvOU._R6ng) - [original paper](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "Whether you're a researcher, student, or professional, this demo notebook will showcase how LangChain can revolutionize your document exploration and information retrieval workflows.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/aiaicaramba/aiai-for-society/blob/aiai-dev/20230926_AI4Soc_Chat_with_Your_Data.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKfWq0yAiQSv"
      },
      "outputs": [],
      "source": [
        "# cleanup mess and setup data dir\n",
        "!rm -drf /content/sample_data\n",
        "!mkdir /content/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ3ah-Fi99HT"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "This code installs several Python packages that are required for the project. Explanations happily generated for you by [Chepetto](https://openai.com/blog/chatgpt).\n",
        "\n",
        "- [`langchain`](<https://python.langchain.com/>) is a package for language modeling and language generation tasks.\n",
        "- [`openai`](<https://openai.com/>) is a package for accessing the OpenAI API, which provides access to various language models and AI tools.\n",
        "- [`pypdf`](<https://pypi.org/project/PyPDF2/>) is a package for working with PDF files in Python.\n",
        "- [`tiktoken`](<https://github.com/openai/tiktoken>) is a package for accessing the TikToken API.\n",
        "- [`faiss-cpu`](<https://github.com/facebookresearch/faiss>) is a package for performing efficient similarity searches on large datasets using the FAISS library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgzZqejc_tHY"
      },
      "outputs": [],
      "source": [
        "%pip install \\\n",
        "langchain \\\n",
        "openai \\\n",
        "pypdf \\\n",
        "faiss-cpu \\\n",
        "pandas \\\n",
        "# datasets \\\n",
        "tiktoken "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g3qnWtL9uoZ"
      },
      "source": [
        "# OpenAI API Key\n",
        "\n",
        "To use the OpenAI API, you need to obtain an API key from the [OpenAI website](https://platform.openai.com/account/api-keys). The API key is a unique identifier that allows you to access the OpenAI API and make requests to it. By setting the 'OPENAI_API_KEY' environment variable, you can securely provide your API key to the code without hardcoding it into the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAlyjKew6n0N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyqRRBW89pef"
      },
      "source": [
        "# Embeddings setup\n",
        "\n",
        "This code initializes an instance of the [OpenAIEmbeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html?highlight=embeddings#langchain.embeddings.OpenAIEmbeddings) class and assigns it to the variable embeddings. An [embedding](https://platform.openai.com/docs/guides/embeddings) is a way to represent words or phrases as numeric vectors, which can be used as input to machine learning models.  The `OpenAIEmbeddings` class provides access to pre-trained word embeddings from OpenAI, which were trained on a large corpus of text data using advanced deep learning techniques.\n",
        "\n",
        "Once you have initialized an instance of the `OpenAIEmbeddings` class, you can use it to obtain the embedding vector for any given chunk of text. This can be useful for a variety of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) tasks, such as sentiment analysis, language translation, and text classification. In this notebook we use it to do [semantic search](https://en.wikipedia.org/wiki/Semantic_search) with a [vector database](https://www.youtube.com/watch?v=klTvEwg3oJ4&ab_channel=Fireship) in this case.\n",
        "\n",
        "## Model\n",
        "\n",
        "| Name | Tokenizer | Max input tokens | Output dimensions |\n",
        "| :--- | :--- | ---: | ---: |\n",
        "| text-embedding-ada-002 | cl100k_base | 8191 | 1536 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywZY3ISvBtG0"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQndn4wG9gmV"
      },
      "source": [
        "# Splitter setup\n",
        "\n",
        "The [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html) is a text splitting tool that takes in a large text document as input and splits it into smaller chunks for downstream processing. Here's what each parameter in the splitter setup means:\n",
        "\n",
        "- `chunk_size`: This parameter specifies the size of each chunk of text that the splitter will output. In this case, the splitter is set up to output chunks of 500 characters each.\n",
        "\n",
        "- `chunk_overlap`: This parameter specifies the number of characters of overlap that each chunk will have with the next chunk. In this case, the splitter is set up to have an overlap of 20 characters between adjacent chunks.\n",
        "\n",
        "- `length_function`: This parameter specifies the function that the splitter will use to calculate the length of the input text. In this case, the `len` function is used, which returns the number of characters in the text.\n",
        "\n",
        "Together, these parameters determine how the input text will be split into smaller chunks. The splitter will output chunks of 500 characters each, with an overlap of 20 characters between adjacent chunks, until the entire input text has been processed. This setup is designed to balance the need for small enough chunks for efficient processing, with enough overlap between chunks to minimize the risk of losing contextual information at the boundaries between chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXNfZfbW-AwH"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrQCZBg9bjo"
      },
      "source": [
        "# Load (and split) documents\n",
        "\n",
        "This code snippet loads PDF files from a directory named \"data/\" using a [PyPDFDirectoryLoader](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html?highlight=PyPDFDirectoryLoader) class from the `langchain.document_loaders` module. The `loader` variable is an instance of `PyPDFDirectoryLoader`, which takes the directory path as an argument.\n",
        "\n",
        "After instantiating the loader, the code calls the `load_and_split` method to load the PDF files from the directory and split their text using the text splitter we created before.\n",
        "\n",
        "## Upload your PDFs\n",
        "Create a folder called 'data' and throw in any number of pdf's you'd like to chat with.\n",
        "\n",
        "> Note; The pdf's will be deleted once you close the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLbGxELv8X4C"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "loader = PyPDFDirectoryLoader(\"data/\")\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "len(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imTLGCkO14Qd"
      },
      "outputs": [],
      "source": [
        "docs[111].page_content # arbitrary chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEAnfzs4oBUx"
      },
      "outputs": [],
      "source": [
        "# Do a little cleanup.\n",
        "# Generated by GPT4 advanced data analysis: https://chat.openai.com/share/237b1e56-349e-4a5c-a007-d862c724b72f\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Step 1: Remove extra spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # Step 2: Normalize punctuation by removing unnecessary spaces before or after\n",
        "    text = re.sub(' ,', ',', text)\n",
        "    text = re.sub(' \\.', '.', text)\n",
        "    text = re.sub(' ;', ';', text)\n",
        "    text = re.sub(' !', '!', text)\n",
        "    text = re.sub(' \\?', '?', text)\n",
        "    text = re.sub('“ ', '“', text)\n",
        "    text = re.sub(' ”', '”', text)\n",
        "\n",
        "    # Step 3: Remove line breaks\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    return text\n",
        "\n",
        "def ultimate_clean_text(text):\n",
        "    # Previous cleaning steps\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Replacing various types of quotes with standard ASCII quotes\n",
        "    text = text.replace('“', '').replace('”', '').replace('‘', \"'\").replace('’', \"'\").replace('www.ztcprep.com', '')\n",
        "\n",
        "    # Replacing non-breaking spaces with standard spaces\n",
        "    text = text.replace(u'\\xa0', u' ')\n",
        "\n",
        "    # Correcting contractions like \"don' t\" to \"don't\"\n",
        "    text = re.sub(r\"(\\w+) \\'\", r\"\\1'\", text)\n",
        "    text = re.sub(r\"\\' t\", \"'t\", text)\n",
        "\n",
        "    # Removing website URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "cleaned_contents = list(map(lambda doc: ultimate_clean_text(doc.page_content), docs))\n",
        "cleaned_contents[111]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq0xnWT79I0B"
      },
      "outputs": [],
      "source": [
        "# Vector store setup\n",
        "\n",
        "This code creates a **Facebook AI Similarity Searck** ([FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)) from the [langchain.vectorstores](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) module.\n",
        "\n",
        "FAISS enables fast similarity search among high-dimensional vectors. In this code, it is used to create a FAISS index for a set of documents. The `from_documents()` method is called on the FAISS class, passing in two arguments: `docs` and `embeddings`. The `docs` parameter is a set of documents that have been previously loaded and split, and the `embeddings` parameter is a pre-trained embedding model that maps the documents to high-dimensional vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umIO18tb8tpz"
      },
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "faiss_index = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osmwb3QL9QiX"
      },
      "outputs": [],
      "source": [
        "# Save the db\n",
        "\n",
        "This code saves the FAISS index created in the previous code cell to disk with the name `faiss_index`. The `save_local()` method is called on the `faiss_index` object, which is the FAISS index created earlier. The `save_local()` method is a utility method provided by the FAISS class to save the index to the local file system.\n",
        "\n",
        "After executing this code, a file named \"faiss_index\" should be created in the current working directory. This file contains the serialized version of the FAISS index, which can be loaded back into memory later using the `FAISS.load_local()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3AVw_PWCEg-"
      },
      "source": [
        "faiss_index.save_local(\"faiss_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3AVw_PWCEg-"
      },
      "outputs": [],
      "source": [
        "# Test the vector store\n",
        "\n",
        "This code performs a similarity search using the FAISS index created earlier and the query string *\"Is a prototyping more than enough for software?\"*.\n",
        "\n",
        "The `similarity_search()` method is called on the faiss_index object with two arguments: the query string and `k=5`, which specifies that the top 5 most similar documents should be returned. The result of the similarity search is stored in the `query_result` variable.\n",
        "\n",
        "The code then iterates over the chunks in the `query_result` list and prints the metadata and page content of each `chunk`. Specifically, it prints the page number and source of the document, along with its page content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5sRA2Fe9Tky"
      },
      "source": [
        "# Test the vector store\n",
        "\n",
        "This code performs a similarity search using the FAISS index created earlier and the query string *\"Is a prototyping more than enough for software?\"*.\n",
        "\n",
        "The `similarity_search()` method is called on the faiss_index object with two arguments: the query string and `k=5`, which specifies that the top 5 most similar documents should be returned. The result of the similarity search is stored in the `query_result` variable.\n",
        "\n",
        "The code then iterates over the chunks in the `query_result` list and prints the metadata and page content of each `chunk`. Specifically, it prints the page number and source of the document, along with its page content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV2Pmvrt_FTO"
      },
      "outputs": [],
      "source": [
        "query_result = faiss_index.similarity_search(\"Who is Harry Potter?\", k=5)\n",
        "\n",
        "for chunk in query_result:\n",
        "    print(str(chunk.metadata[\"page\"]) + \" \" + chunk.metadata[\"source\"] + \" :\", chunk.page_content[:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncBRUXkX8xrH"
      },
      "source": [
        "# Chat memory\n",
        "\n",
        "This code imports the [ConversationBufferWindowMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer_window.html) class from the `langchain.memory` module and creates an instance of it called `memory`. This class represents a memory buffer that stores conversations in a windowed fashion, meaning that the buffer only retains a certain number of recent conversations.\n",
        "\n",
        "The constructor of the `ConversationBufferWindowMemory` class takes two arguments: `memory_key` and `return_messages`. The `memory_key` parameter specifies a unique identifier for the memory buffer, and the `return_messages` parameter indicates whether or not to return the stored messages along with their metadata when accessing the memory buffer.\n",
        "\n",
        "In this code, the `memory_key` is set to \"chat_history\", which is being used to store the chat conversations. The return_messages parameter is set to `True`, which indicates that the stored messages will be returned along with their metadata when accessing the memory buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWbEn5NlBWK7"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkpI1EY8t02"
      },
      "source": [
        "# Chain setup\n",
        "\n",
        "This code imports several classes and functions from various modules in the langchain package and creates an instance of the [ConversationalRetrievalChain](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html?highlight=ConversationalRetrievalChain) class called `qa`.\n",
        "\n",
        "The `ConversationalRetrievalChain` class is a high-level class that provides an interface for building a conversational agent that can perform retrieval-based question answering. In this code, the `qa` instance is initialized using the `from_llm()` method, which initializes the agent using an LLM model, a retriever and the memory buffer.\n",
        "\n",
        "### LLM\n",
        "The `OpenAI` class from the `langchain.llms` module represents an instance of the OpenAI language model. In this code, an instance of the OpenAI class is created of the model \"[gpt-3.5-turbo](https://platform.openai.com/docs/models)\".\n",
        "\n",
        "### Vector Store\n",
        "The `faiss_index.as_retriever()` method returns a retriever instance that wraps the FAISS index created earlier. This retriever is used to retrieve candidate answers to questions asked of the conversational agent.\n",
        "\n",
        "### Chat History Memory\n",
        "The `memory` variable is a memory buffer that was created earlier using the `ConversationBufferWindowMemory` class. This memory buffer is used to store and retrieve past conversations for use in future interactions.\n",
        "\n",
        "The `verbose=True` parameter indicates that verbose output should be produced when running the conversational agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--P_eVczGm1X"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=500),\n",
        "    faiss_index.as_retriever(k=5),\n",
        "    memory=memory, verbose=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "source": [
        "queries = ['Which school does Harry Potter attend and why?',\n",
        "         'How did Harry found out about his powers and what are they?',\n",
        "         'How did he become friends with the other students?',\n",
        "         'Describe the character Hermione.']\n",
        "\n",
        "\n",
        "chat_result = qa({\"question\": queries[1]})\n",
        "chat_result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "outputs": [],
      "source": [
        "queries = ['Which school does Harry Potter attend and why?',\n",
        "         'How did Harry found out about his powers and what are they?',\n",
        "         'How did he become friends with the other students?',\n",
        "         'Describe the character Hermione.']\n",
        "\n",
        "\n",
        "chat_result = qa({\"question\": queries[1]})\n",
        "chat_result['answer']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
