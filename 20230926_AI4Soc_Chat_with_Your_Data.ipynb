{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAhBW4_JFsv"
      },
      "source": [
        "# Chat based document Q&A with LangChain and OpenAI API\n",
        "\n",
        "This notebook will guide you through the process of setting up the environment, importing documents, and interacting with LangChain for document-based Q&A. We'll cover topics such as document preprocessing, question formulation, and analyzing the model's responses. This technique is called [Retrieval Augmented Generation (RAG)](https://www.perplexity.ai/search/What-is-Retrieval-s56y5BPbQGyxsvOU._R6ng) - [original paper](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "Whether you're a researcher, student, or professional, this demo notebook will showcase how LangChain can revolutionize your document exploration and information retrieval workflows.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/aiaicaramba/aiai-for-society/blob/aiai-dev/20230926_AI4Soc_Chat_with_Your_Data.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ3ah-Fi99HT"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "This code installs several Python packages that are required for the project. Explanations happily generated for you by [Chepetto](https://openai.com/blog/chatgpt).\n",
        "\n",
        "- [`langchain`](<https://python.langchain.com/>) is a package for language modeling and language generation tasks.\n",
        "- [`openai`](<https://openai.com/>) is a package for accessing the OpenAI API, which provides access to various language models and AI tools.\n",
        "- [`pypdf`](<https://pypi.org/project/PyPDF2/>) is a package for working with PDF files in Python.\n",
        "- [`tiktoken`](<https://github.com/openai/tiktoken>) is a package for accessing the TikToken API.\n",
        "- [`faiss-cpu`](<https://github.com/facebookresearch/faiss>) is a package for performing efficient similarity searches on large datasets using the FAISS library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dgzZqejc_tHY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Using cached langchain-0.0.327-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting openai\n",
            "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pypdf\n",
            "  Using cached pypdf-3.17.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.7.4-cp311-cp311-macosx_10_9_x86_64.whl (6.5 MB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.1.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.5.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain)\n",
            "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Using cached SQLAlchemy-2.0.22-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.4 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Using cached aiohttp-3.8.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting anyio<4.0 (from langchain)\n",
            "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Using cached dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
            "  Using cached langsmith-0.0.56-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numpy<2,>=1 (from langchain)\n",
            "  Using cached numpy-1.26.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
            "Collecting pydantic<3,>=1 (from langchain)\n",
            "  Using cached pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
            "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting tqdm (from openai)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting pyarrow>=8.0.0 (from datasets)\n",
            "  Using cached pyarrow-13.0.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.4.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Using cached huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting packaging (from datasets)\n",
            "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken)\n",
            "  Using cached regex-2023.10.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached multidict-6.0.4-cp311-cp311-macosx_10_9_x86_64.whl (29 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached yarl-1.9.2-cp311-cp311-macosx_10_9_x86_64.whl (64 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached frozenlist-1.4.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting idna>=2.8 (from anyio<4.0->langchain)\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting sniffio>=1.1 (from anyio<4.0->langchain)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting filelock (from huggingface-hub<1.0.0,>=0.14.0->datasets)\n",
            "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0.0,>=0.14.0->datasets)\n",
            "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
            "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1->langchain)\n",
            "  Using cached pydantic_core-2.10.1-cp311-cp311-macosx_10_7_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
            "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
            "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Using cached greenlet-3.0.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.7 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Using cached langchain-0.0.327-py3-none-any.whl (2.0 MB)\n",
            "Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "Using cached pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "Using cached pandas-2.1.2-cp311-cp311-macosx_10_9_x86_64.whl (11.6 MB)\n",
            "Using cached datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "Using cached tiktoken-0.5.1-cp311-cp311-macosx_10_9_x86_64.whl (953 kB)\n",
            "Using cached aiohttp-3.8.6-cp311-cp311-macosx_10_9_x86_64.whl (365 kB)\n",
            "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "Using cached dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "Using cached huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached langsmith-0.0.56-py3-none-any.whl (44 kB)\n",
            "Using cached numpy-1.26.1-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
            "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "Using cached pyarrow-13.0.0-cp311-cp311-macosx_10_14_x86_64.whl (25.8 MB)\n",
            "Using cached pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
            "Using cached pydantic_core-2.10.1-cp311-cp311-macosx_10_7_x86_64.whl (1.9 MB)\n",
            "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "Using cached PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl (187 kB)\n",
            "Using cached regex-2023.10.3-cp311-cp311-macosx_10_9_x86_64.whl (296 kB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached SQLAlchemy-2.0.22-cp311-cp311-macosx_10_9_x86_64.whl (2.1 MB)\n",
            "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "Using cached xxhash-3.4.1-cp311-cp311-macosx_10_9_x86_64.whl (31 kB)\n",
            "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl (121 kB)\n",
            "Using cached frozenlist-1.4.0-cp311-cp311-macosx_10_9_x86_64.whl (47 kB)\n",
            "Using cached greenlet-3.0.1-cp311-cp311-macosx_10_9_universal2.whl (263 kB)\n",
            "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
            "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, faiss-cpu, xxhash, urllib3, tzdata, typing-extensions, tqdm, tenacity, sniffio, six, regex, PyYAML, pypdf, packaging, numpy, mypy-extensions, multidict, jsonpointer, idna, greenlet, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, python-dateutil, pydantic-core, pyarrow, multiprocess, marshmallow, jsonpatch, anyio, aiosignal, tiktoken, pydantic, pandas, huggingface-hub, dataclasses-json, aiohttp, openai, langsmith, langchain, datasets\n",
            "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.22 aiohttp-3.8.6 aiosignal-1.3.1 annotated-types-0.6.0 anyio-3.7.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.2 dataclasses-json-0.6.1 datasets-2.14.6 dill-0.3.7 faiss-cpu-1.7.4 filelock-3.13.1 frozenlist-1.4.0 fsspec-2023.10.0 greenlet-3.0.1 huggingface-hub-0.18.0 idna-3.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.327 langsmith-0.0.56 marshmallow-3.20.1 multidict-6.0.4 multiprocess-0.70.15 mypy-extensions-1.0.0 numpy-1.26.1 openai-0.28.1 packaging-23.2 pandas-2.1.2 pyarrow-13.0.0 pydantic-2.4.2 pydantic-core-2.10.1 pypdf-3.17.0 python-dateutil-2.8.2 pytz-2023.3.post1 regex-2023.10.3 requests-2.31.0 six-1.16.0 sniffio-1.3.0 tenacity-8.2.3 tiktoken-0.5.1 tqdm-4.66.1 typing-extensions-4.8.0 typing-inspect-0.9.0 tzdata-2023.3 urllib3-2.0.7 xxhash-3.4.1 yarl-1.9.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \\\n",
        "langchain \\\n",
        "openai \\\n",
        "pypdf \\\n",
        "faiss-cpu \\\n",
        "pandas \\\n",
        "datasets \\\n",
        "tiktoken "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g3qnWtL9uoZ"
      },
      "source": [
        "# OpenAI API Key\n",
        "\n",
        "To use the OpenAI API, you need to obtain an API key from the [OpenAI website](https://platform.openai.com/account/api-keys). The API key is a unique identifier that allows you to access the OpenAI API and make requests to it. By setting the 'OPENAI_API_KEY' environment variable, you can securely provide your API key to the code without hardcoding it into the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DAlyjKew6n0N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyqRRBW89pef"
      },
      "source": [
        "# Embeddings setup\n",
        "\n",
        "This code initializes an instance of the [OpenAIEmbeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html?highlight=embeddings#langchain.embeddings.OpenAIEmbeddings) class and assigns it to the variable embeddings. An [embedding](https://platform.openai.com/docs/guides/embeddings) is a way to represent words or phrases as numeric vectors, which can be used as input to machine learning models.  The `OpenAIEmbeddings` class provides access to pre-trained word embeddings from OpenAI, which were trained on a large corpus of text data using advanced deep learning techniques.\n",
        "\n",
        "Once you have initialized an instance of the `OpenAIEmbeddings` class, you can use it to obtain the embedding vector for any given chunk of text. This can be useful for a variety of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) tasks, such as sentiment analysis, language translation, and text classification. In this notebook we use it to do [semantic search](https://en.wikipedia.org/wiki/Semantic_search) with a [vector database](https://www.youtube.com/watch?v=klTvEwg3oJ4&ab_channel=Fireship) in this case.\n",
        "\n",
        "## Model\n",
        "\n",
        "| Name | Tokenizer | Max input tokens | Output dimensions |\n",
        "| :--- | :--- | ---: | ---: |\n",
        "| text-embedding-ada-002 | cl100k_base | 8191 | 1536 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ywZY3ISvBtG0"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQndn4wG9gmV"
      },
      "source": [
        "# Splitter setup\n",
        "\n",
        "The [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html) is a text splitting tool that takes in a large text document as input and splits it into smaller chunks for downstream processing. Here's what each parameter in the splitter setup means:\n",
        "\n",
        "- `chunk_size`: This parameter specifies the size of each chunk of text that the splitter will output. In this case, the splitter is set up to output chunks of 500 characters each.\n",
        "\n",
        "- `chunk_overlap`: This parameter specifies the number of characters of overlap that each chunk will have with the next chunk. In this case, the splitter is set up to have an overlap of 20 characters between adjacent chunks.\n",
        "\n",
        "- `length_function`: This parameter specifies the function that the splitter will use to calculate the length of the input text. In this case, the `len` function is used, which returns the number of characters in the text.\n",
        "\n",
        "Together, these parameters determine how the input text will be split into smaller chunks. The splitter will output chunks of 500 characters each, with an overlap of 20 characters between adjacent chunks, until the entire input text has been processed. This setup is designed to balance the need for small enough chunks for efficient processing, with enough overlap between chunks to minimize the risk of losing contextual information at the boundaries between chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oXNfZfbW-AwH"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrQCZBg9bjo"
      },
      "source": [
        "# Load (and split) documents\n",
        "\n",
        "This code snippet loads PDF files from a directory named \"data/\" using a [PyPDFDirectoryLoader](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html?highlight=PyPDFDirectoryLoader) class from the `langchain.document_loaders` module. The `loader` variable is an instance of `PyPDFDirectoryLoader`, which takes the directory path as an argument.\n",
        "\n",
        "After instantiating the loader, the code calls the `load_and_split` method to load the PDF files from the directory and split their text using the text splitter we created before.\n",
        "\n",
        "## Upload your PDFs\n",
        "Create a folder called 'data' and throw in any number of pdf's you'd like to chat with.\n",
        "\n",
        "> Note; The pdf's will be deleted once you close the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PLbGxELv8X4C"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "loader = PyPDFDirectoryLoader(\"data/\")\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "len(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "imTLGCkO14Qd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'17.3 Ik een beetje meer dan jij…\\nJe verdeelt een geldbedrag over vier personen (A, B, C en D). Elke persoon krijgt een ander\\nbedrag: A krijgt 20% meer dan het gemiddelde van de vier personen. Voor de duidelijkheid:\\nuiteindelijk krijgen de vier personen samen het hele bedrag. A krijgt daarvan een kwart +\\n20% van zo’n kwart. Daarna verlaat persoon A de kamer.\\nEr blijven drie personen over, met wie een soortgelijke verdeling plaatsvindt: B krijgt 20%\\nmeer dan het gemiddelde van de 3 overgebleven personen en verlaat de kamer. C krijgt 20%\\nmeer dan het gemiddelde van de 2 overgebleven personen. D ontvangt de overgebleven 168\\neuro.\\nHet oorspronkelijke, te verdelen bedrag is … euro.\\nSchrijf een computerprogramma dat dit getal berekent.\\nHieronder een deel van de oplossing, maar probeer het eerst zelf! (Afkomstig van beterreken.nl,\\n12aug2020; een som van Henk van Huffelen)\\n17.4 Ik een beetje meer dan jij… aanzet tot een oplossing'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[111].page_content # arbitrary chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zEAnfzs4oBUx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"17.3 Ik een beetje meer dan jij… Je verdeelt een geldbedrag over vier personen (A, B, C en D). Elke persoon krijgt een ander bedrag: A krijgt 20% meer dan het gemiddelde van de vier personen. Voor de duidelijkheid: uiteindelijk krijgen de vier personen samen het hele bedrag. A krijgt daarvan een kwart + 20% van zo'n kwart. Daarna verlaat persoon A de kamer. Er blijven drie personen over, met wie een soortgelijke verdeling plaatsvindt: B krijgt 20% meer dan het gemiddelde van de 3 overgebleven personen en verlaat de kamer. C krijgt 20% meer dan het gemiddelde van de 2 overgebleven personen. D ontvangt de overgebleven 168 euro. Het oorspronkelijke, te verdelen bedrag is … euro. Schrijf een computerprogramma dat dit getal berekent. Hieronder een deel van de oplossing, maar probeer het eerst zelf! (Afkomstig van beterreken.nl, 12aug2020; een som van Henk van Huffelen) 17.4 Ik een beetje meer dan jij… aanzet tot een oplossing\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Do a little cleanup.\n",
        "# Generated by GPT4 advanced data analysis: https://chat.openai.com/share/237b1e56-349e-4a5c-a007-d862c724b72f\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Step 1: Remove extra spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # Step 2: Normalize punctuation by removing unnecessary spaces before or after\n",
        "    text = re.sub(' ,', ',', text)\n",
        "    text = re.sub(' \\.', '.', text)\n",
        "    text = re.sub(' ;', ';', text)\n",
        "    text = re.sub(' !', '!', text)\n",
        "    text = re.sub(' \\?', '?', text)\n",
        "    text = re.sub('“ ', '“', text)\n",
        "    text = re.sub(' ”', '”', text)\n",
        "\n",
        "    # Step 3: Remove line breaks\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    return text\n",
        "\n",
        "def ultimate_clean_text(text):\n",
        "    # Previous cleaning steps\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Replacing various types of quotes with standard ASCII quotes\n",
        "    text = text.replace('“', '').replace('”', '').replace('‘', \"'\").replace('’', \"'\").replace('www.ztcprep.com', '')\n",
        "\n",
        "    # Replacing non-breaking spaces with standard spaces\n",
        "    text = text.replace(u'\\xa0', u' ')\n",
        "\n",
        "    # Correcting contractions like \"don' t\" to \"don't\"\n",
        "    text = re.sub(r\"(\\w+) \\'\", r\"\\1'\", text)\n",
        "    text = re.sub(r\"\\' t\", \"'t\", text)\n",
        "\n",
        "    # Removing website URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "cleaned_contents = list(map(lambda doc: ultimate_clean_text(doc.page_content), docs))\n",
        "cleaned_contents[111]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq0xnWT79I0B"
      },
      "source": [
        "# Vector store setup\n",
        "\n",
        "This code creates a **Facebook AI Similarity Searck** ([FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)) from the [langchain.vectorstores](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) module.\n",
        "\n",
        "FAISS enables fast similarity search among high-dimensional vectors. In this code, it is used to create a FAISS index for a set of documents. The `from_documents()` method is called on the FAISS class, passing in two arguments: `docs` and `embeddings`. The `docs` parameter is a set of documents that have been previously loaded and split, and the `embeddings` parameter is a pre-trained embedding model that maps the documents to high-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umIO18tb8tpz"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "faiss_index = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osmwb3QL9QiX"
      },
      "source": [
        "# Save the db\n",
        "\n",
        "This code saves the FAISS index created in the previous code cell to disk with the name `faiss_index`. The `save_local()` method is called on the `faiss_index` object, which is the FAISS index created earlier. The `save_local()` method is a utility method provided by the FAISS class to save the index to the local file system.\n",
        "\n",
        "After executing this code, a file named \"faiss_index\" should be created in the current working directory. This file contains the serialized version of the FAISS index, which can be loaded back into memory later using the `FAISS.load_local()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "M3AVw_PWCEg-"
      },
      "outputs": [],
      "source": [
        "faiss_index.save_local(\"faiss_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5sRA2Fe9Tky"
      },
      "source": [
        "# Test the vector store\n",
        "\n",
        "This code performs a similarity search using the FAISS index created earlier and the query string *\"Is a prototyping more than enough for software?\"*.\n",
        "\n",
        "The `similarity_search()` method is called on the faiss_index object with two arguments: the query string and `k=5`, which specifies that the top 5 most similar documents should be returned. The result of the similarity search is stored in the `query_result` variable.\n",
        "\n",
        "The code then iterates over the chunks in the `query_result` list and prints the metadata and page content of each `chunk`. Specifically, it prints the page number and source of the document, along with its page content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lV2Pmvrt_FTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77 data/Programming-Basics.pdf : Bronnen: Een aantal van deze opgaven komen van www.beterrekenen.nl (ook als mobile app):\n",
            "Elke dag een paar minuten aan uitdagingen, soms makkelijk, soms moeilijker, some recht-\n",
            "toe-recht-aan, soms echt nodig om een plaatje te tekenen of een laptop te hulp te roepen…\n",
            "hersengym dus… een aanrader!\n",
            "17.7 Spoilers…\n",
            "Enkele antwoorden:\n",
            "•Al wandelend... : 4746 meter\n",
            "78\n",
            "116 data/Programming-Basics.pdf : abelen (en namen van lokale variabelen beginnen met een kleineletter).\n",
            "•DeMain-method is de enige plek waar Console.ReadLine() enConsole.WriteLine()\n",
            "voorkomt!\n",
            "•Roep elke methode meerdere keren aan! Bijvoorbeeld een methode die 2 strings als\n",
            "parameter heeft roep je minstens 1 keer aan met 2 strings van verschillende lengte, maar\n",
            "ook met 2 strings van gelijke lengte. Controleer het antwoord!\n",
            "•Dehappy flow testen we tijdens het programmeren meestal wel. Probeer ook uitzon-\n",
            "deringssituaties te testen! Geef eens een lege string mee aan een methode die een string-\n",
            "parameter heeft, en een methode die een List of een array verwacht wordt aangeroepen\n",
            "met een lege List of array.\n",
            "42.2 Eerste Hulp Bij Vasthangen\n",
            "Hoe pulk je letters uit een string? Hoe tel je het aantal letters in een string? Hoe kijk je of\n",
            "een string als deel van een andere string voorkomt? + string manipulatie (in Toolbox basis)\n",
            "117\n",
            "25 data/Programming-Basics.pdf : Step Into laat regel voor regel door de code stappen. Dit betekent dat je ook in een methode\n",
            "kunt gaan die worden aangeroepen.\n",
            "Stap met F11 door deze code heen. In het Command Window zie je het resultaat hiervan\n",
            "weergegeven.\n",
            "Step Over laat je ook stap voor stap door de code lopen maar gaat niet de methodes in. Dat\n",
            "is handig als je alleen het resultaat van een methode wilt zien. Dat kan is onze code niet\n",
            "bekeken worden, we roepen namelijk geen andere methode aan.\n",
            "26\n",
            "26 data/Programming-Basics.pdf : Step Out is handig als je niet langer door een methode wilt lopen. Dat heb je nodig als je\n",
            "in een systeemfunctie bent beland. Maar ook als je gewoon snel naar het volgende statement\n",
            "wilt kun je deze knop gebruiken.\n",
            "Continue is handig als je tijdens het debuggen gezien hebt wat je moet zien. Dus als je al\n",
            "gezien hebt dat het goed gaat. Als je op Continue klikt gaat de debugger naar het volgende\n",
            "breakpoint of naar het einde van het programma als er geen breakpoint meer is.\n",
            "5.3 Breakpoints\n",
            "BreakPoints zijn erg belangrijk bij debuggen. Je kunt een BreakPoint ook zetten door\n",
            "éénmaal op de grijze verticale balk aan de linkerkant van je scherm te klikken.\n",
            "BreakPoints opent het BreakPoint Window en integreert het samen met andere windows\n",
            "onderaan in de Visual Studio omgeving. Dit window bevat informatie over alle BreakPoints\n",
            "die op dit moment zijn gezet.\n",
            "Zet in je programma ook nog een BreakPoint op methodenaam “Main” en bekijk het\n",
            "BreakPoints Window .\n",
            "27\n",
            "73 data/Programming-Basics.pdf : 17 Variabelen, bewerkingen en conversies:\n",
            "T raining Computer-rekenen\n",
            "Als de computer dan toch zo’n krachtige calculator is, laten we hem dan ook als zodanig\n",
            "gebruiken…\n",
            "17.1 Recht-toe-recht-aan\n",
            "•100 x 0,7 - 25 x 0,6 = …\n",
            "•18.0 / 5.0 = …\n",
            "•100 / 30 = …\n",
            "Schrijf een programma dat deze antwoorden berekent.\n",
            "Als je het op een rekenmachine (bijvoorbeeld de calculator-app op je laptop) berekent: komt\n",
            "er dan dezelfde uitkomst uit? Zo nee, probeer erachter te komen waarom dat zo is.\n",
            "17.2 Al wandelend…\n",
            "Patrick maakt een wandeling van A naar B. De afstand is 20 km.\n",
            "Hij begint in A en als hij 1/4 deel van de afstand heeft afgelegd, rust hij een poosje (rustpunt\n",
            "1). Na korte tijd hervat hij de wandeling en loopt 1/4 deel van het resterende gedeelte, Daar\n",
            "rust hij weer even uit (rustpunt 2). Op dezelfde manier wandelt hij verder. Elke keer rust hij\n",
            "een poosje als hij weer 1/4 deel van het resterende gedeelte heeft gelopen.\n"
          ]
        }
      ],
      "source": [
        "query_result = faiss_index.similarity_search(\"Wat is een loop?\", k=5)\n",
        "\n",
        "for chunk in query_result:\n",
        "    print(str(chunk.metadata[\"page\"]) + \" \" + chunk.metadata[\"source\"] + \" :\", chunk.page_content[:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncBRUXkX8xrH"
      },
      "source": [
        "# Chat memory\n",
        "\n",
        "This code imports the [ConversationBufferWindowMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer_window.html) class from the `langchain.memory` module and creates an instance of it called `memory`. This class represents a memory buffer that stores conversations in a windowed fashion, meaning that the buffer only retains a certain number of recent conversations.\n",
        "\n",
        "The constructor of the `ConversationBufferWindowMemory` class takes two arguments: `memory_key` and `return_messages`. The `memory_key` parameter specifies a unique identifier for the memory buffer, and the `return_messages` parameter indicates whether or not to return the stored messages along with their metadata when accessing the memory buffer.\n",
        "\n",
        "In this code, the `memory_key` is set to \"chat_history\", which is being used to store the chat conversations. The return_messages parameter is set to `True`, which indicates that the stored messages will be returned along with their metadata when accessing the memory buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hWbEn5NlBWK7"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkpI1EY8t02"
      },
      "source": [
        "# Chain setup\n",
        "\n",
        "This code imports several classes and functions from various modules in the langchain package and creates an instance of the [ConversationalRetrievalChain](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html?highlight=ConversationalRetrievalChain) class called `qa`.\n",
        "\n",
        "The `ConversationalRetrievalChain` class is a high-level class that provides an interface for building a conversational agent that can perform retrieval-based question answering. In this code, the `qa` instance is initialized using the `from_llm()` method, which initializes the agent using an LLM model, a retriever and the memory buffer.\n",
        "\n",
        "### LLM\n",
        "The `OpenAI` class from the `langchain.llms` module represents an instance of the OpenAI language model. In this code, an instance of the OpenAI class is created of the model \"[gpt-3.5-turbo](https://platform.openai.com/docs/models)\".\n",
        "\n",
        "### Vector Store\n",
        "The `faiss_index.as_retriever()` method returns a retriever instance that wraps the FAISS index created earlier. This retriever is used to retrieve candidate answers to questions asked of the conversational agent.\n",
        "\n",
        "### Chat History Memory\n",
        "The `memory` variable is a memory buffer that was created earlier using the `ConversationBufferWindowMemory` class. This memory buffer is used to store and retrieve past conversations for use in future interactions.\n",
        "\n",
        "The `verbose=True` parameter indicates that verbose output should be produced when running the conversational agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "--P_eVczGm1X"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0.7, max_tokens=500),\n",
        "    faiss_index.as_retriever(k=5),\n",
        "    memory=memory, verbose=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "source": [
        "Voorbeeld: \n",
        "queries = ['Which school does Harry Potter attend and why?',\n",
        "         'How did Harry found out about his powers and what are they?',\n",
        "         'How did he become friends with the other students?',\n",
        "         'Describe the character Hermione.']\n",
        "\n",
        "chat_result = qa({\"question\": queries[0]})  # De 1e vraag wordt beantwoord (want index begint bij 0)\n",
        "chat_result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some example queries: \n",
            "0 : Hoe maak ik een loop die start bij 3 en loopt tot 25 0 Hoe maak ik een loop die start bij 3 en loopt tot 25\n",
            "1 : hoe verander ik het returntype van een methode? 1 hoe verander ik het returntype van een methode?\n",
            "2 : Kan ik in C# ook modulo berekenen? 2 Kan ik in C# ook modulo berekenen?\n",
            "3 : Hoe bepaal ik in C# of een getal deelbaar is door 7? 3 Hoe bepaal ik in C# of een getal deelbaar is door 7?\n",
            "\n",
            "Your input: [graagg een voorbeeld waarvoor ik een enum zou gebruiken]\n",
            "Ah, you typed a nice question.\n",
            "Q: graagg een voorbeeld waarvoor ik een enum zou gebruiken\n",
            "Ja, natuurlijk! Een enum (enumeration) in C# wordt gebruikt om een set van constanten te definiëren die een specifieke betekenis hebben. Hier is een voorbeeld:\n",
            "\n",
            "enum Weekdag\n",
            "{\n",
            "    Maandag,\n",
            "    Dinsdag,\n",
            "    Woensdag,\n",
            "    Donderdag,\n",
            "    Vrijdag,\n",
            "    Zaterdag,\n",
            "    Zondag\n",
            "}\n",
            "\n",
            "Met deze enum kun je bijvoorbeeld de dagen van de week representeren. Je kunt dan een variabele van het type Weekdag declareren en een specifieke dag toewijzen:\n",
            "\n",
            "Weekdag dag = Weekdag.Maandag;\n",
            "\n",
            "Je kunt ook een switch statement gebruiken om verschillende acties uit te voeren op basis van de waarde van de enum:\n",
            "\n",
            "switch (dag)\n",
            "{\n",
            "    case Weekdag.Maandag:\n",
            "        Console.WriteLine(\"Het is maandag!\");\n",
            "        break;\n",
            "    case Weekdag.Dinsdag:\n",
            "        Console.WriteLine(\"Het is dinsdag!\");\n",
            "        break;\n",
            "    // ...\n",
            "    default:\n",
            "        Console.WriteLine(\"Het is geen bekende dag.\");\n",
            "        break;\n",
            "}\n",
            "\n",
            "Het gebruik van een enum maakt de code leesbaarder en voorkomt het gebruik van magische getallen.\n"
          ]
        }
      ],
      "source": [
        "queries = ['Hoe maak ik een loop die start bij 3 en loopt tot 25',\n",
        "         'hoe verander ik het returntype van een methode?',\n",
        "         'Kan ik in C# ook modulo berekenen?',\n",
        "         'Hoe bepaal ik in C# of een getal deelbaar is door 7?']\n",
        "\n",
        "print(\"Some example queries: \")\n",
        "for nr,q in enumerate(queries):\n",
        "    print(f\"{ nr } : {q}\",nr,q)\n",
        "\n",
        "\n",
        "q = input(\"Typ number of query, or type in your own query, then press <enter>\")\n",
        "print(f\"\\nYour input: [{q}]\")\n",
        "if q.isdigit():\n",
        "    q = int(q)\n",
        "    if q in range(len(queries)):\n",
        "        q = queries[q]\n",
        "    else:\n",
        "        print(\"I don't know what question that would be...\")\n",
        "else:\n",
        "    print(\"Ah, you typed a nice question.\")\n",
        "    # q probably already contains the query typed by the user.\n",
        "\n",
        "# q is a string containing the chosen query\n",
        "\n",
        "print(f\"Q: {q}\")\n",
        "chat_result = qa({\"question\": q})\n",
        "print(chat_result['answer'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
